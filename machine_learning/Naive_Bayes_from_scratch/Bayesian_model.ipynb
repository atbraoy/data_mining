{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from 'pima-indians-diabetes.csv' with size: 768\n",
      "Training set size: 529, Test set size: 239\n",
      "Classes found in the dataset: [0.0, 1.0]\n",
      "Data has a class key of 0 with count of 337\n",
      "Data has a class key of 1 with count of 192\n",
      "Classes found in the dataset: [0.0, 1.0]\n",
      "Data has a class key of 0 with count of 500\n",
      "Data has a class key of 1 with count of 268\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-* \n",
      "Sample of summary:\n",
      "Attributes:[6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 0.627, 50.0, 1.0], mean = 38.4696666667, stdev = 48.2961124833\n",
      "Attributes:[1.0, 85.0, 66.0, 29.0, 0.0, 26.6, 0.351, 31.0, 0.0], mean = 26.5501111111, stdev = 31.1197437347\n",
      "Attributes:[8.0, 183.0, 64.0, 0.0, 0.0, 23.3, 0.672, 32.0, 1.0], mean = 34.6635555556, stdev = 59.585319952\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "{0.0: [(3.9262759924385633, 3.3360192106287285), (120.77693761814744, 32.11283606046778), (68.8468809073724, 20.725871209961873), (20.75047258979206, 15.602123397508596), (81.15879017013232, 117.92272975331343), (31.86616257088847, 7.921973218716789), (0.4727032136105859, 0.319711299706638), (33.139886578449904, 11.625602655704503)], 1.0: [(3.9262759924385633, 3.3360192106287285), (120.77693761814744, 32.11283606046778), (68.8468809073724, 20.725871209961873), (20.75047258979206, 15.602123397508596), (81.15879017013232, 117.92272975331343), (31.86616257088847, 7.921973218716789), (0.4727032136105859, 0.319711299706638), (33.139886578449904, 11.625602655704503)]}\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"We will follow the steps below to execute this tutorial:\n",
    "\n",
    "* Prepare the Data: Load the data from a file and split it into training and test datasets. One can also load directly from the online database but this should be covered in a different tutorial.\n",
    "* Split the Data: We split our data, using a given ratio, to create our training set and test set.\n",
    "* Classify our Data:  We divide the training data into classes (using the given ones) such that we can associate probabilities with each class.\n",
    "* Build the Model: We use the Gaussian distribution function (equation above) to create our probability calculator. This require to calculate the mean $\\mu$ and the standard deviation $\\sigma$ of each set of attributes and then we associate that with the given classes. \n",
    "* Predict: After training our model we use the outcome (the summary/result of the Gaussian model evaluation for each class), we generate predictions given the test the model outcome.\n",
    "* Measure Accuracy: We check how accurate is our model\n",
    "* Compare Accuracy: We compare the prediction made by our model to that made by 'scikit-learn' model.\"\"\"\n",
    "\n",
    "import os, csv, random, math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "from pandas import Series\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Data_Manipulate(object):\n",
    "    def __init__(self, _folder, _file):\n",
    "        self._folder = _folder\n",
    "        self._file = _file\n",
    "        #self._dataset = _dataset\n",
    "        \n",
    "    def data_loader(self,):\n",
    "        _dir = os.path.join(os.path.dirname(os.path.realpath(\"__file__\")), self._folder)\n",
    "        file_path = _dir+'/'+self._file\n",
    "        data = csv.reader(open(file_path, \"rb\"))\n",
    "        dataset = list(data)\n",
    "        for line in range(len(dataset)):\n",
    "            dataset[line] = [float(x) for x in dataset[line]] \n",
    "        dataset = dataset[:-1]\n",
    "        print \"Data loaded from '{0}' with size: {1}\".format(self._file, len(dataset))\n",
    "    \n",
    "        return self._file, dataset\n",
    "    \n",
    "    def data_splitter(self, _dataset, split_ratio):\n",
    "        train_size = int(len(_dataset)*split_ratio)\n",
    "        train_set = []\n",
    "        test_set = list(_dataset)\n",
    "        while len(train_set) < train_size:\n",
    "            index = random.randrange(len(test_set))\n",
    "            train_set.append(test_set.pop(index))\n",
    "        print \"Training set size: {0}, Test set size: {1}\".format(abs(len(train_set)), len(test_set))\n",
    "        \n",
    "        return [train_set, test_set]\n",
    "    \n",
    "class Data_Classify(object):\n",
    "    \n",
    "    \"\"\"Classifications: a simple scenario that we take each vector from our dataset (i.e. row by row) and \n",
    "    and test it against our argument of classification: the vectors that contain large number of \n",
    "    elements that are a like will be put in same class, and so on\"\"\"\n",
    "    \n",
    "    def __init__(self, _dataset):\n",
    "        self._dataset = _dataset\n",
    "        \n",
    "    def data_class(self, train_set):\n",
    "        _class = {}\n",
    "        for x in range(len(self._dataset)):\n",
    "            _row = self._dataset[x]\n",
    "            if (_row[-1] not in _class): \n",
    "                #[-1] means takes the last element and classify accordingly\n",
    "                # the reason of that because our 'class:0,1' is at the end of the row\n",
    "                _class[_row[-1]] = []\n",
    "            _class[_row[-1]].append(_row)\n",
    "            \n",
    "        print \"Classes found in the dataset: {0}\".format(_class.keys())\n",
    "        for i in range(len(_class.keys())):\n",
    "            print \"Data has a class key of {0} with count of {1}\".format(i, \n",
    "                [item[-1] for item in train_set].count(_class.keys()[i]))\n",
    "       \n",
    "        return _class\n",
    "\n",
    "class Data_Summary(object):\n",
    "    \n",
    "    \"\"\"- Here are our estimation tools, the mean and the standard deviation \n",
    "    - We put the training data, after measuring the mean and standard deviation, \n",
    "    into summary class, such that we will have [(mean_1, stdev_1), ... (mean_n, stdev_n)]\n",
    "    - Separat the training dataset into instances grouped by class, then calculate the \n",
    "    summaries (which decrib the mean and stdev, from above fucntion) for each attribute \n",
    "    this should give {class: (mean, stdev)} such that: \n",
    "    {0: [(m1, st1), (m2, st2), ...], 1: [(m1, st1), (m2, st2), ...]}\"\"\"\n",
    "    \n",
    "    def __init__(self, _dataset):\n",
    "        #Data_Classify.__init__(self, _dataset)\n",
    "        self._dataset = _dataset\n",
    "        \n",
    "    #@staticmethod\n",
    "    def mean(self, enteries):\n",
    "        enteries = enteries#[:-1]\n",
    "        mean = sum(enteries)/float(len(enteries))\n",
    "        #print mean\n",
    "    \n",
    "        return mean\n",
    "\n",
    "    #@staticmethod\n",
    "    def stdev(self, enteries):\n",
    "        enteries = enteries#[:-1]\n",
    "        _mean = self.mean(enteries)\n",
    "        variance = sum([pow(x-_mean,2) for x in enteries])/float(len(enteries)-1)\n",
    "        _stdev = math.sqrt(variance)\n",
    "    \n",
    "        return _stdev\n",
    "    #@classmethod\n",
    "    def data_summary(self,):\n",
    "        summary = [(self.mean(attribute), self.stdev(attribute)) for attribute in zip(*self._dataset)]\n",
    "        del summary[-1]\n",
    "\n",
    "#         print '-*'*25, '\\nSummary sample:'\n",
    "#         for i in range(len(summary[:3])): \n",
    "#             print('Attributes:{0}, mean = {1}, stdev = {2}').format(\n",
    "#                 self._dataset[i], \n",
    "#                 self.mean(self._dataset[i]), \n",
    "#                 self.stdev(self._dataset[i]))\n",
    "#         print '-*'*25\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def sort_data(self, dataset):\n",
    "        init = Data_Classify(dataset)\n",
    "        separated = init.data_class(dataset)\n",
    "        classes = {}\n",
    "        for classValue, instances in separated.iteritems():\n",
    "            classes[classValue] = self.data_summary()\n",
    "            \n",
    "        print '-*'*25, '\\nSample of summary:'\n",
    "        for i in range(len(dataset[:3])): \n",
    "            print('Attributes:{0}, mean = {1}, stdev = {2}').format(\n",
    "                dataset[i], \n",
    "                self.mean(dataset[i]), \n",
    "                self.stdev(dataset[i]))\n",
    "\n",
    "        #print('\\nData sorted by class value: {0}').format(classes)\n",
    "        print '-*'*25\n",
    "        \n",
    "        return classes\n",
    "\n",
    "class Probability_Calculator(object):\n",
    "    \"\"\"\n",
    "    All that being given, we are now ready to carry our predictions using the training set. \n",
    "    The process is now simple, we just need to allow for this scenario: calculate the probability \n",
    "    that a given set of attributes will belong to a specific class, then we mark the class with \n",
    "    the highest probability scor as our prediction. In steps:\n",
    "        Calculate Gaussian Probability Density Function per class,\n",
    "        Calculate all the probabilities for all classes,\n",
    "        Predict,\n",
    "        Measure the model accuracy.\n",
    "        \"\"\"\n",
    "#     def __init__(self, _trainSet, _testSet):\n",
    "#         self._trainSet = _trainSet\n",
    "#         self._testSet = _testSet\n",
    "        \n",
    "    def gaussian_estimator(self, x, mean, stdev):\n",
    "        base = math.exp(-(math.pow(x-mean,2)/(2*math.pow(stdev,2))))\n",
    "        estimator = (1.0 / (math.sqrt(2*math.pi) * stdev)) * base\n",
    "    \n",
    "        return estimator\n",
    "    \n",
    "    def class_probability(self, train, test):\n",
    "        probabilities = {}\n",
    "        for target, vector in train.iteritems():\n",
    "            probabilities[target] = 1 # we need to have initial value here.\n",
    "            for i in range(len(vector)):\n",
    "                mean, stdev = vector[i]\n",
    "                x = test[i]\n",
    "            probabilities[target] *= self.gaussian_estimator(x, mean, stdev)\n",
    "            \n",
    "        return probabilities\n",
    "    \n",
    "    def predict(self, train_set, test_set):\n",
    "        predictions = []\n",
    "        for i in range(len(test_set)):\n",
    "            probabilities = self.class_probability(train_set, test_set[i])\n",
    "            _class, _prob = None, -1\n",
    "            for Class, probability in probabilities.iteritems():\n",
    "                if _class is None or probability > _prob:\n",
    "                    _prob = probability\n",
    "                    _class = Class\n",
    "            predictions.append(_class)\n",
    "            \n",
    "        print predictions\n",
    "        return predictions\n",
    "    \n",
    "#     def class_probability(self, trainSet, testSet):\n",
    "#         probabilities = {}\n",
    "#         for target, vector in trainSet.iteritems():\n",
    "#             probabilities[target] = 1 # we need to have initial value here.\n",
    "#             for i in range(len(vector)):\n",
    "#                 mean, stdev = vector[i]\n",
    "#                 x = self._testSet[i]\n",
    "#                 probabilities[target] *= self.gaussian_estimator(x, mean, stdev)\n",
    "         \n",
    "#         return probabilities\n",
    "    \n",
    "#     def predict(self, trainSet, testSet):\n",
    "#         predictions = []\n",
    "#         for i in range(len(testSet)):\n",
    "#             print testSet[i]\n",
    "#             probabilities = self.class_probability(trainSet, testSet[i])\n",
    "#             _class, _prob = None, -1\n",
    "#             for Class, probability in probabilities.iteritems():\n",
    "#                 if _class is None or probability > _prob:\n",
    "#                     _prob = probability\n",
    "#                     _class = Class\n",
    "#             predictions.append(_class)\n",
    "            \n",
    "#         print predictions\n",
    "#         return predictions\n",
    "    \n",
    "        \n",
    "# self test:---\n",
    "if __name__ == '__main__':\n",
    "    file_ = 'pima-indians-diabetes.csv'\n",
    "    folder_ = 'data'\n",
    "    data_manipulate = Data_Manipulate(folder_, file_)\n",
    "    _file, _dataset = data_manipulate.data_loader()\n",
    "    split_ratio = 0.69\n",
    "    train_set, test_set = data_manipulate.data_splitter(_dataset, split_ratio)\n",
    "    data_classify = Data_Classify(_dataset)\n",
    "    data_classify.data_class(train_set)\n",
    "    \n",
    "    summary = Data_Summary(train_set)  \n",
    "    summary.data_summary()\n",
    "    sorted_data = summary.sort_data(_dataset)\n",
    "    print sorted_data\n",
    "    test= [[1.0, 85.0, 66.0, 29.0, 0.0, 26.6, 0.351, 31.0,'?'],\n",
    "            [6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 0.627, 50.0, '?'], \n",
    "        [5.0, 116.0, 74.0, 0.0, 0.0, 25.6, 0.201, 30.0, '?'], \n",
    "        [3.0, 78.0, 50.0, 32.0, 88.0, 31.0, 0.248, 26.0, '?']]\n",
    "    trained = {0.0: [(3.1729106628242074, 2.922716172887118), (108.71469740634005, 25.20303421516836), (67.54755043227665, 18.547234296564014), (19.80979827089337, 14.796055431011643), (69.28530259365995, 98.47869924672679), (30.52247838616713, 7.398200192897232), (0.4184351585014407, 0.2815707967066099), (30.746397694524497, 11.60646657969856)], 1.0: [(4.791208791208791, 3.665627963256114), (139.33516483516485, 32.8364905279732), (70.47802197802197, 21.672271676077802), (23.13186813186813, 17.853429521021233), (96.63736263736264, 132.61459270582085), (35.89505494505495, 6.814076083365414), (0.5798076923076926, 0.37589898335371674), (36.043956043956044, 10.462151821080543)]}\n",
    "    probabilities = Probability_Calculator()\n",
    "        #probability = probabilities.class_probability()\n",
    "    probabilities.predict(sorted_data, test_set)\n",
    "        \n",
    "        #print(\"Attribute's probability: {0}\" ).format(probability)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
