{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pyspark import SparkContext, sql, SparkConf\n",
    "import nltk\n",
    "import __builtin__\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "\n",
    "#import pyspark.sql as sparksql\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "#-------------------------------\n",
    "# Importing from other folders, appending the path\n",
    "sys.path.append('../')\n",
    "from spark_sessions import start_session, stop_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------- manipulating the data \n",
    "class Data_manipulate(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.sample = sample\n",
    "        self.keys = keys\n",
    "        self.types = types\n",
    "        self.schema = schema\n",
    "        #self.framed = framed\n",
    "        \n",
    "        # --- functions:\n",
    "        self.panda_framing()\n",
    "        self.spark_framing()\n",
    "        self.create_schema()\n",
    "        self.prettySummary()\n",
    "        self.spark_local_session()\n",
    "    \n",
    "    #----------------\n",
    "    def structure_field(self, key, item): # item = StringType() ...\n",
    "        key = str\n",
    "        item = str\n",
    "        structure = []\n",
    "        structure.append(StructField(key, item, True))\n",
    "        \n",
    "        return structure\n",
    "    \n",
    "    #----------------\n",
    "    def prettySummary(self):\n",
    "        \"\"\" Neat summary statistics of a Spark dataframe\n",
    "        Args:\n",
    "            pyspark.sql.dataframe.DataFrame (df): input dataframe\n",
    "        Returns:\n",
    "            pandas.core.frame.DataFrame: a pandas dataframe with the summary statistics of df\n",
    "        \"\"\"\n",
    "        #import pandas as pd\n",
    "        temp = self.framed.describe().toPandas()\n",
    "        temp.iloc[1:3,1:] = temp.iloc[1:3,1:].convert_objects(convert_numeric=True)\n",
    "        pd.options.display.float_format = '{:,.2f}'.format\n",
    "        \n",
    "        return temp\n",
    "    \n",
    "    \n",
    "    #----------------\n",
    "    def panda_framing(self):\n",
    "        # Frame the data with panda\n",
    "        _panda_framed = pd.read_csv(sample)\n",
    "        panda_framed = pd.DataFrame(_panda_framed)\n",
    "        \n",
    "        return panda_framed\n",
    "    \n",
    "    #----------------\n",
    "    def spark_local_session(self):\n",
    "#         conf = (SparkConf()\n",
    "#           .setAppName(\"experiment-airbnb\")\n",
    "#           .set(\"spark.executor.instances\", \"10\")\n",
    "#           .set(\"spark.executor.cores\", 2)\n",
    "#           .set(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "#           .set(\"spark.shuffle.service.enabled\", \"false\")\n",
    "#           .set(\"spark.executor.memory\", \"500MB\"))\n",
    "#         sc = SparkContext.getOrCreate(conf = conf)\n",
    "        spark = SparkSession.builder \\\n",
    "            .master(\"yarn\") \\\n",
    "            .appName(\"experiment-airbnb\") \\\n",
    "            .enableHiveSupport() \\\n",
    "            .getOrCreate()\n",
    "\n",
    "        sc = SQLContext(spark) # Double check this!!!\n",
    "        \n",
    "        return sc\n",
    "    #----------------\n",
    "    def spark_framing(self):    \n",
    "        host = \"local[2]\"\n",
    "        app = \"experiment-airbnb\"\n",
    "        memory_id = \"spark.executor.memory\"\n",
    "        memory_size = \"500m\"      \n",
    "        spark = start_session(host, app,  memory_id, memory_size)\n",
    "        \n",
    "        #sqlContext = self.spark_local_session()\n",
    "        \n",
    "        spark = SparkSession.builder \\\n",
    "            .master(\"yarn\") \\\n",
    "            .appName(\"experiment-airbnb\") \\\n",
    "            .enableHiveSupport() \\\n",
    "            .getOrCreate()\n",
    "\n",
    "        sqlContext = SQLContext(spark)\n",
    "        \n",
    "        sc = SparkContext.getOrCreate()\n",
    "#        sqlContext = SQLContext(spark) \n",
    "#         sqlContext = SQLContext(sc) \n",
    "\n",
    "        \n",
    "        if sample.lower().endswith(('.csv', '.txt')):\n",
    "            #sparked_data = spark.read.format('com.databricks.spark.csv')\\\n",
    "            #.options(header='true', delimiter=',')\\\n",
    "            #.load(sample, schema = schema).cache() ## You need to check 'schema' here! \n",
    "            sparked_data = spark.read.load(sample, \n",
    "                              format='com.databricks.spark.csv', \n",
    "                              header='true', \n",
    "                             inferSchema='true').cache()\n",
    "            print \"Found data file with extension '.csv'/'txt'\"\n",
    "            print \"Using '.load(data).cache()' method.\"\n",
    "        else:\n",
    "            sparked_data = self.spark_local_session().textFile(sample)\n",
    "            print \"Found format undetermined!\"\n",
    "        \n",
    "        raw_data = sc.textFile(sample)\n",
    "        \n",
    "        \n",
    "        _raw_schema = raw_data.first().replace('\"','') # Creating a schema\n",
    "        _schema = [str(key) for key in _raw_schema.split(',')] # mapping\n",
    "        \n",
    "        fields = []\n",
    "        for key in range(len(self.keys)):\n",
    "            if self.types[key] == str:\n",
    "                data_type = StringType()\n",
    "                str_struct = StructField(_schema[key], data_type, True)\n",
    "                fields.append(str_struct)\n",
    "                #print self.types[key]\n",
    "            elif self.types[key] == int:\n",
    "                data_type = DoubleType()\n",
    "                int_struct = StructField(_schema[key], data_type, True)\n",
    "                fields.append(int_struct) \n",
    "                #print self.types[key]\n",
    "        \n",
    "        headers = raw_data.filter(lambda l: \"_id\" in l)\n",
    "        NoHeaders = raw_data.subtract(headers)\n",
    "        schema = StructType(fields)\n",
    "        spark_dframed = NoHeaders.toDF(schema)    # Spark dataframe\n",
    "        #panda_dframed = spark_dframed.toPandas() # also to pandas\n",
    "        ## register data frame as a temporary table\n",
    "        spark_dframed.createOrReplaceTempView(\"test\")\n",
    "        #re=sqlContext.sql(\"select max_seq from test\")\n",
    "        #print(re.show())\n",
    "        #results = sqlContext.sql(\"select first(name), name, first(host_id), concat_ws(';', collect_list(aspect)) as aspect from temp group by host_id\")\n",
    "        spark_dframed.printSchema()\n",
    "#         results = sqlContext.sql(\"SELECT product, count(*) AS total_count FROM id GROUP BY product ORDER BY total_count DESC\")\n",
    "\n",
    "#         for x in results.collect():\n",
    "#             print x\n",
    "        \n",
    "        #-----------------------------------\n",
    "        #_items = {}\n",
    "        #_rows = []\n",
    "        #for key in range(len(self.keys)):\n",
    "        #    item = self.keys[key]\n",
    "        #    _items[self.keys[key]] = key\n",
    "        #_rows.append(_items)\n",
    "        #rdd = sc.parallelize(_rows)\n",
    "        \n",
    "        #sqlContext = SQLContext(self.spark_local_session())\n",
    "        #sqlContext.createDataFrame(rdd, schema).collect()\n",
    "        #-----------------------------------\n",
    "        \n",
    "        #print df.describe().dtypes #df.describe().show()\n",
    "        #print \"Schema for this data is: \\n\", sparked_data.printSchema()\n",
    "        #print df.select(\"host_name\").show()\n",
    "        #print df.groupBy(\"host_name\").count().show()\n",
    "        #print df.groupBy(\"neighbourhood\").count().show()\n",
    "        self.framed = sparked_data\n",
    "        #print self.prettySummary()\n",
    "        #return sparked_data\n",
    "    \n",
    "    #----------------\n",
    "    def create_schema(self):\n",
    "        # Frame the data with panda, take first row of data\n",
    "        first_row = self.panda_framing().iloc[0]\n",
    "        #print \"Number of lines in your 'sample' file:\", len(self.panda_framing().index) \n",
    "        _keys = []\n",
    "        for key in first_row.index:#dict(framed_data.dtypes):\n",
    "            #if dict(framed_data.dtypes)[key] in ['float64', 'int64']:\n",
    "            _keys.append(key)\n",
    "        self.keys = _keys\n",
    "\n",
    "        _types = []\n",
    "        for _type in first_row.values:\n",
    "            value = np.array(_type).tolist()\n",
    "            #if not type(x) == str:\n",
    "            _types.append(type(value))\n",
    "  \n",
    "        self.types = _types \n",
    "        \n",
    "        struct_list = []\n",
    "        for key in range(len(self.keys)):\n",
    "                #keys = self.keys[key]\n",
    "                if self.types[key] == str:\n",
    "                    data_type = StringType()\n",
    "                    str_struct = StructField(keys, data_type, True)\n",
    "                    struct_list.append(str_struct)\n",
    "                    #print self.types[key]\n",
    "                elif self.types[key] == int:\n",
    "                    data_type = DoubleType()\n",
    "                    int_struct = StructField(keys, data_type, True)\n",
    "                    struct_list.append(int_struct) \n",
    "                    #print self.types[key]\n",
    "        \n",
    "        self.schema = StructType(struct_list)\n",
    "        #print self.schema\n",
    "        \n",
    "             \n",
    "        return self.schema\n",
    "    \n",
    "    #----------------\n",
    "    def spark_schema(self):\n",
    "        self.spark_framing()\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "# # Creating a pandas dataframe from Sample Data\n",
    "# pd_dataframe = pd.read_csv(sample)\n",
    "# sc = SparkContext()\n",
    "# sqlContext = SQLContext(sc)\n",
    "# sql_context = sql.SQLcontext(sc)\n",
    "\n",
    "# # # Creating a Spark DataFrame from a pandas dataframe\n",
    "# spark_df = sql_context.createDataFrame(df)\n",
    "\n",
    "# spark_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session started ...\n",
      "Found data file with extension '.csv'/'txt'\n",
      "Using '.load(data).cache()' method.\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "u\"Error while instantiating 'org.apache.spark.sql.hive.HiveSessionState':\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7e3a65d6d49e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#schema = Data_manipulate(sample, keys).create_schema()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmanipulate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mData_manipulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanipulate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# map(manipulate.create_schema(sample), (keys))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#print schema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-24e12a4c70d9>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# --- functions:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpanda_framing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_framing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprettySummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-24e12a4c70d9>\u001b[0m in \u001b[0;36mspark_framing\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mNoHeaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mspark_dframed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNoHeaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# Spark dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m#panda_dframed = spark_dframed.toPandas() # also to pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m## register data frame as a temporary table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/spark/1.0.1/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/spark/1.0.1/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_schema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Ahmed/anaconda/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/spark/1.0.1/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: u\"Error while instantiating 'org.apache.spark.sql.hive.HiveSessionState':\""
     ]
    }
   ],
   "source": [
    "# self execution/testing\n",
    "if __name__ == '__main__':\n",
    "    sample = \"/Users/Ahmed/Documents/DataMining_Stuff/Hadoop/Spark/PySpark/data/airbnb/sample/sample.csv\"\n",
    "    keys = \"\"\n",
    "    types = \"\"\n",
    "    schema = \"\"\n",
    "    #schema = Data_manipulate(sample, keys).create_schema()\n",
    "    manipulate = Data_manipulate()\n",
    "    schema = manipulate.create_schema()# map(manipulate.create_schema(sample), (keys))\n",
    "    #print schema\n",
    "    manipulate.spark_schema()\n",
    "    #print \"Schema:\", schema\n",
    "# data.dtypes\n",
    "# [key for key in dict(data.dtypes) if dict(data.dtypes)[key] in ['float64', 'int64']]\n",
    "# for key in dict(data.dtypes):\n",
    "#     if dict(data.dtypes)[key] in ['float64', 'int64']:\n",
    "#         print key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bool"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = True\n",
    "type(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Ahmed/Documents/DataMining_Stuff/Hadoop/Spark/PySpark/tutorials/airbnb\n"
     ]
    }
   ],
   "source": [
    "print os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct the schema from the header \n",
    ">>> header = taxiFile.first()\n",
    ">>> header\n",
    "u'\"_id\",\"_rev\",\"dropoff_datetime\",\"dropoff_latitude\",\"dropoff_longitude\",\"hack_license\",\"medallion\",\"passenger_count\",\"pickup_datetime\",\"pickup_latitude\",\"pickup_longitude\",\"rate_code\",\"store_and_fwd_flag\",\"trip_distance\",\"trip_time_in_secs\",\"vendor_id\"'\n",
    ">>> schemaString = header.replace('\"','')  # get rid of the double-quotes\n",
    ">>> schemaString\n",
    "u'_id,_rev,dropoff_datetime,dropoff_latitude,dropoff_longitude,hack_license,medallion,passenger_count,pickup_datetime,pickup_latitude,pickup_longitude,rate_code,store_and_fwd_flag,trip_distance,trip_time_in_secs,vendor_id'\n",
    ">>> fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split(',')]\n",
    ">>> schema = StructType(fields)\n",
    "\n",
    "# Subtract header and use the above-constructed schema:\n",
    ">>> taxiHeader = taxiFile.filter(lambda l: \"_id\" in l) # taxiHeader needs to be an RDD - the string we constructed above will not do the job\n",
    ">>> taxiHeader.collect() # for inspection purposes only\n",
    "[u'\"_id\",\"_rev\",\"dropoff_datetime\",\"dropoff_latitude\",\"dropoff_longitude\",\"hack_license\",\"medallion\",\"passenger_count\",\"pickup_datetime\",\"pickup_latitude\",\"pickup_longitude\",\"rate_code\",\"store_and_fwd_flag\",\"trip_distance\",\"trip_time_in_secs\",\"vendor_id\"']\n",
    ">>> taxiNoHeader = taxiFile.subtract(taxiHeader)\n",
    ">>> taxi_df = taxiNoHeader.toDF(schema)  # Spark dataframe\n",
    ">>> import pandas as pd\n",
    ">>> taxi_DF = taxi_df.toPandas()  # pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "...     .master(\"local\") \\\n",
    "...     .appName(\"Word Count\") \\\n",
    "...     .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "...     .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "u\"Error while instantiating 'org.apache.spark.sql.hive.HiveSessionState':\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-d99b92b61045>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#df = spark.createDataFrame(rdd).collect()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#spark.createDataFrame(rdd, ['name', 'age']).collect()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# #df.createOrReplaceTempView(\"table1\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# sqll.registerDataFrameAsTable(df, \"table1\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/spark/1.0.1/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/spark/1.0.1/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_schema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Ahmed/anaconda/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/spark/1.0.1/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: u\"Error while instantiating 'org.apache.spark.sql.hive.HiveSessionState':\""
     ]
    }
   ],
   "source": [
    "l = [('Alice', 1)]\n",
    "spark.createDataFrame(l).collect()\n",
    "spark.createDataFrame(l, ['name', 'age']).collect()\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqll = SQLContext(spark)\n",
    "rdd = sc.parallelize(l)\n",
    "#df = spark.createDataFrame(rdd).collect()\n",
    "df = rdd.spark.createDataFrame(rdd, ['name', 'age']).collect()\n",
    "# #df.createOrReplaceTempView(\"table1\")\n",
    "# sqll.registerDataFrameAsTable(df, \"table1\")\n",
    "#sqlContext = SQLContext(sc)\n",
    "\n",
    "sqlContext = SQLContext(sparkContext=spark.sparkContext, sparkSession=spark)\n",
    "sqlContext.registerDataFrameAsTable(df, \"table1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
