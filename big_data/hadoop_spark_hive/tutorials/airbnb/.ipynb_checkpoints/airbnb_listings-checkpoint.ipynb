{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pyspark import SparkContext, sql\n",
    "import nltk\n",
    "import __builtin__\n",
    "import numpy as np\n",
    "\n",
    "#import pyspark.sql as sparksql\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "#-------------------------------\n",
    "# Importing from other folders, appending the path\n",
    "sys.path.append('../')\n",
    "from spark_sessions import start_session, stop_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- manipulating the data \n",
    "class Data_manipulate(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.sample = sample\n",
    "        self.keys = keys\n",
    "        self.types = types\n",
    "        self.schema = schema\n",
    "        self.framed = framed\n",
    "        \n",
    "        # --- functions:\n",
    "        self.panda_framing()\n",
    "        self.spark_framing()\n",
    "        self.create_schema()\n",
    "        self.prettySummary()\n",
    "    \n",
    "    #----------------\n",
    "    def structure_field(self, key, item): # item = StringType() ...\n",
    "        key = str\n",
    "        item = str\n",
    "        structure = []\n",
    "        structure.append(StructField(key, item, True))\n",
    "        \n",
    "        return structure\n",
    "    \n",
    "    #----------------\n",
    "    def prettySummary(self):\n",
    "        \"\"\" Neat summary statistics of a Spark dataframe\n",
    "        Args:\n",
    "            pyspark.sql.dataframe.DataFrame (df): input dataframe\n",
    "        Returns:\n",
    "            pandas.core.frame.DataFrame: a pandas dataframe with the summary statistics of df\n",
    "        \"\"\"\n",
    "        #import pandas as pd\n",
    "        temp = self.framed.describe().toPandas()\n",
    "        temp.iloc[1:3,1:] = temp.iloc[1:3,1:].convert_objects(convert_numeric=True)\n",
    "        pd.options.display.float_format = '{:,.2f}'.format\n",
    "        return temp\n",
    "    \n",
    "    \n",
    "    #----------------\n",
    "    def panda_framing(self):\n",
    "        # Frame the data with panda\n",
    "        _panda_framed = pd.read_csv(sample)\n",
    "        panda_framed = pd.DataFrame(_panda_framed)\n",
    "        \n",
    "        return panda_framed\n",
    "    \n",
    "    #----------------\n",
    "    def spark_framing(self):\n",
    "        host = \"local[2]\"\n",
    "        app = \"experiment-airbnb\"\n",
    "        memory_id = \"spark.executor.memory\"\n",
    "        memory_size = \"512m\"      \n",
    "        sparked_data = start_session(host, app,  memory_id, memory_size, sample)\n",
    "        \n",
    "#         #sc = SparkContext.getOrCreate()\n",
    "#         #sqlContext = SQLContext(sc) \n",
    "        \n",
    "#         sparked_data = spark.read.load(sample, \n",
    "#                           format='com.databricks.spark.csv', \n",
    "#                           header='true', \n",
    "#                           inferSchema='true').cache()\n",
    "#         data = spark.textFile(sample).cache()\n",
    "#         csv_data = raw_data.map(lambda l: l.split(\",\"))\n",
    "#         row_data = csv_data.map(lambda p: Row(\n",
    "#             duration=int(p[0]), \n",
    "#             protocol_type=p[1],\n",
    "#             service=p[2],\n",
    "#             flag=p[3],\n",
    "#             src_bytes=int(p[4]),\n",
    "#             dst_bytes=int(p[5])))\n",
    "        \n",
    "        #read.format('com.databricks.spark.csv').options(header='true').load(sample)\n",
    "#         df.take(5)\n",
    "        #print df.describe().dtypes #df.describe().show()\n",
    "        print \"Schema for this data is: \\n\", data#sparked_data.printSchema()\n",
    "        #print df.select(\"host_name\").show()\n",
    "        #print df.groupBy(\"host_name\").count().show()\n",
    "        #print df.groupBy(\"neighbourhood\").count().show()\n",
    "        self.framed = sparked_data\n",
    "        #print self.prettySummary()\n",
    "        return sparked_data\n",
    "    \n",
    "    #----------------\n",
    "    def create_schema(self):\n",
    "        # Frame the data with panda, take first row of data\n",
    "        first_row = self.panda_framing().iloc[0]\n",
    "        print \"Number of lines in your 'sample' file:\", len(self.panda_framing().index) \n",
    "        _keys = []\n",
    "        for key in first_row.index:#dict(framed_data.dtypes):\n",
    "            #if dict(framed_data.dtypes)[key] in ['float64', 'int64']:\n",
    "            _keys.append(key)\n",
    "        self.keys = _keys\n",
    "        #print \"-------------------------\"\n",
    "        #print \"Keys:\", self.keys#.translate(None, \"<\")\n",
    "        \n",
    "        i = 0\n",
    "        _types = []\n",
    "        for _type in first_row.values:\n",
    "            value = np.array(_type).tolist()\n",
    "            #if not type(x) == str:\n",
    "            _types.append(type(value))\n",
    "            i = +i\n",
    "        self.types = _types \n",
    "        #print \"-------------------------\"\n",
    "        #print \"Items:\", self.types#.translate(None, \"<\")\n",
    "        \n",
    "        #for item in self.items:\n",
    "        struct_list = []\n",
    "        for key in range(len(self.keys)):\n",
    "                keys = self.keys[key]\n",
    "                if self.types[key] == str:\n",
    "                    data_type = StringType()\n",
    "                    str_struct = StructField(keys, data_type, True)\n",
    "                    struct_list.append(str_struct)\n",
    "                    #print _schema\n",
    "                elif self.types[key] == int:\n",
    "                    data_type = DoubleType()\n",
    "                    int_struct = StructField(keys, data_type, True)\n",
    "                    struct_list.append(int_struct) \n",
    "        \n",
    "        self.schema = StructType(struct_list)\n",
    "        #print self.keys\n",
    "        \n",
    "             \n",
    "        return self.schema\n",
    "    \n",
    "    #----------------\n",
    "    def spark_schema(self, sample):\n",
    "        self.spark_framing()\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "# # Creating a pandas dataframe from Sample Data\n",
    "# pd_dataframe = pd.read_csv(sample)\n",
    "# sc = SparkContext()\n",
    "# sqlContext = SQLContext(sc)\n",
    "# sql_context = sql.SQLcontext(sc)\n",
    "\n",
    "# # # Creating a Spark DataFrame from a pandas dataframe\n",
    "# spark_df = sql_context.createDataFrame(df)\n",
    "\n",
    "# spark_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session started ...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SQLContext' object has no attribute 'textFile'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-57738a8046eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mframed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#schema = Data_manipulate(sample, keys).create_schema()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmanipulate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mData_manipulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m#schema = manipulate.create_schema(sample)# map(manipulate.create_schema(sample), (keys))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#print schema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-0e69e34e7ad7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# --- functions:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpanda_framing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_framing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprettySummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-0e69e34e7ad7>\u001b[0m in \u001b[0;36mspark_framing\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m                           \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                           inferSchema='true').cache()\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;31m#         csv_data = raw_data.map(lambda l: l.split(\",\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m#         row_data = csv_data.map(lambda p: Row(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SQLContext' object has no attribute 'textFile'"
     ]
    }
   ],
   "source": [
    "# self execution/testing\n",
    "if __name__ == '__main__':\n",
    "    sample = \"/Users/Ahmed/Documents/DataMining_Stuff/Hadoop/Spark/PySpark/data/airbnb/sample/sample.csv\"\n",
    "    keys = \"\"\n",
    "    types = \"\"\n",
    "    schema = \"\"\n",
    "    framed = \"\"\n",
    "    #schema = Data_manipulate(sample, keys).create_schema()\n",
    "    manipulate = Data_manipulate()\n",
    "    #schema = manipulate.create_schema(sample)# map(manipulate.create_schema(sample), (keys))\n",
    "    #print schema\n",
    "    manipulate.spark_schema(sample)\n",
    "    #print \"Schema:\", schema\n",
    "# data.dtypes\n",
    "# [key for key in dict(data.dtypes) if dict(data.dtypes)[key] in ['float64', 'int64']]\n",
    "# for key in dict(data.dtypes):\n",
    "#     if dict(data.dtypes)[key] in ['float64', 'int64']:\n",
    "#         print key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bool"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = True\n",
    "type(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Ahmed/Documents/DataMining_Stuff/Hadoop/Spark/PySpark/tutorials/airbnb\n"
     ]
    }
   ],
   "source": [
    "print os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
