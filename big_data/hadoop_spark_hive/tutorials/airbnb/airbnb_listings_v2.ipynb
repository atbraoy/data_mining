{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pyspark import SparkContext, sql, SparkConf\n",
    "import nltk\n",
    "import re\n",
    "from datetime import datetime\n",
    "from itertools import islice\n",
    "import __builtin__\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "\n",
    "#import pyspark.sql as sparksql\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "#-------------------------------\n",
    "# Importing from other folders, appending the path\n",
    "sys.path.append('../')\n",
    "from spark_sessions import start_session, stop_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RDD' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-188-ef374d69ad70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;31m# spark_data_txt(sample)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;31m# spark_data_sql(sample)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m \u001b[0mresidual_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;31m#spark_schema_v0(sample)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-188-ef374d69ad70>\u001b[0m in \u001b[0;36mresidual_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mresidual_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark_data_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"_id\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RDD' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "spark_configs = (SparkConf()\\\n",
    "            .setAppName(\"experiment-airbnb\")\\\n",
    "            .set(\"spark.executor.instances\", \"10\")\\\n",
    "            .set(\"spark.executor.cores\", 2)\\\n",
    "            .set(\"spark.dynamicAllocation.enabled\", \"false\")\\\n",
    "            .set(\"spark.shuffle.service.enabled\", \"false\")\\\n",
    "            .set(\"spark.executor.memory\", \"500MB\"))\n",
    "\n",
    "#----------------\n",
    "def panda_framing(self):\n",
    "    # Frame the data with panda\n",
    "    _panda_framed = pd.read_csv(sample)\n",
    "    panda_framed = pd.DataFrame(_panda_framed)\n",
    "    \n",
    "    return panda_framed\n",
    "\n",
    "#----------------\n",
    "def spark_session(version):\n",
    "    if version == 0:\n",
    "        conf = spark_configs\n",
    "        return conf\n",
    "    \n",
    "    elif version == 1:\n",
    "        spark = SparkSession.builder \\\n",
    "            .master(\"yarn\") \\\n",
    "            .appName(\"experiment-airbnb\") \\\n",
    "            .enableHiveSupport() \\\n",
    "            .getOrCreate()\n",
    "            \n",
    "        return spark\n",
    "            \n",
    "\n",
    "#----------------\n",
    "def spark_data_txt(sample):\n",
    "    # Creating \"pyspark.context.SparkContext object\"\n",
    "    conf = spark_session(version=0)\n",
    "    sc = SparkContext.getOrCreate(conf = conf)\n",
    "    data = sc.textFile(sample)\n",
    "    #print 'type of context for session v 0:', sc\n",
    "    return data\n",
    "    \n",
    "#----------------\n",
    "def spark_data_sql(sample):\n",
    "    # Creating \"pyspark.sql.context.SQLContext object\"\n",
    "    sc = SQLContext(spark_session(version=1))\n",
    "    data = sc.read.load(sample,\n",
    "                    format='com.databricks.spark.csv', \n",
    "                    header='true', \n",
    "                    inferSchema='true').cache()\n",
    "    #print 'type of context for session v 1:', sc\n",
    "        \n",
    "    return data\n",
    "\n",
    "#----------------\n",
    "def structure_field(key, item): # item = StringType() ...\n",
    "    key = str\n",
    "    item = str\n",
    "    structure = []\n",
    "    structure.append(StructField(key, item, True))\n",
    "        \n",
    "    return structure\n",
    "\n",
    "#----------------\n",
    "def spark_schema_v1(sample):\n",
    "    data = spark_data_txt(sample)\n",
    "    _keys = data.take(2)\n",
    "#     keys_list = []\n",
    "#     for _key in _keys[1:]:\n",
    "#         print _key.split(',')\n",
    "#     keys_list.append(_key)\n",
    "    \n",
    "#         #print \"keys:\", nltk.word_tokenize(_key)#_key.replace('\"','')#.replace(',','')\n",
    "        #print  u', '.join(_keys[1:])\n",
    "            \n",
    "    #print \"token\", nltk.word_tokenize(_keys[1:])#.replace('\"','')\n",
    "    schemaString = data.first().replace('\"','') # Creating a schema\n",
    "    #print schemaString\n",
    "    #for i in schemaString.split(','): print i\n",
    "    fields = [StructField(field_name, StringType(), True) \n",
    "              for field_name in schemaString.split(',')\n",
    "             ]   \n",
    "    _schema = [str(key) for key in schemaString.split(',')] # mapping\n",
    "    #print fields\n",
    "\n",
    "#----------------\n",
    "def residual_data():\n",
    "    data = spark_data_txt(sample)\n",
    "    print data.columns()\n",
    "    headers = data.filter(lambda l: \"_id\" in l)\n",
    "    _data = data.subtract(headers)\n",
    "#     _temp_data = _data.\\\n",
    "#         map(lambda k: k.split(\",\")).\\\n",
    "#         map(lambda p: (\n",
    "#             p[0], p[1], \n",
    "#             parse(p[2].strip('\"')), \n",
    "#             float(p[3]), \n",
    "#             float(p[4]) , \n",
    "#             p[5], \n",
    "#             p[6] , \n",
    "#             int(p[7]), \n",
    "#             parse(p[8].strip('\"')), \n",
    "#             float(p[9]), \n",
    "#             float(p[10]), \n",
    "#             int(p[11]), \n",
    "#             p[12], \n",
    "#             float(p[13]), \n",
    "#             int(p[14]), \n",
    "#             p[15] ))\n",
    "# >>> taxi_temp.top(2)  # have a look:\n",
    "#     print _data.take(2)\n",
    "#     _spark_data = _data.subtract(headers)\n",
    "#     spark_data= _spark_data.map(lambda l: l.split(\",\"))\n",
    "\n",
    "    \n",
    "#----------------\n",
    "def spark_schema_v0(sample):\n",
    "    data = spark_data_sql(sample) # we need this to create the keys (data types)\n",
    "    _keys = data.take(2)\n",
    "    \n",
    "    _data = spark_data_txt(sample) # we need this to create the schema\n",
    "    schemaString = _data.first().replace('\"','')\n",
    "    _schema = schemaString.split(',')#data.columns # Creating a schema\n",
    "    headers = _data.filter(lambda l: \"_id\" in l) # Headers needs to be an RDD - the string we constructed above will not do the job\n",
    "    headers.collect()\n",
    "    _spark_data = _data.subtract(headers)\n",
    "    spark_data= _spark_data.map(lambda l: l.split(\",\"))\n",
    "\n",
    "    \n",
    "    print \"Found %d columns in your data file \\n\" % len(_keys[0]) \n",
    "    _types = []\n",
    "    fields = []\n",
    "    #schem = schemaString.split(',')\n",
    "    print \"Followings are the details of your schema:\"\n",
    "    print \"------------------------------------------\"\n",
    "    for key in range(len(_keys[1])) or (type(_keys[1][key]) == None):   \n",
    "        if (type(_keys[1][key]) == int):\n",
    "            data_type = IntegerType()\n",
    "            str_struct = StructField(_schema[key], data_type, True)\n",
    "            fields.append(str_struct)\n",
    "            #_types.append((_schema[key], data_type))\n",
    "            print 'key:', '\"'+_schema[key]+'\",',',', ' value:', _keys[1][key],',', ' data type:', data_type #, ' value:', _keys[0][key], \n",
    "        \n",
    "        elif (type(_keys[1][key]) == float):\n",
    "            data_type = FloatType()\n",
    "            str_struct = StructField(_schema[key], data_type, True)\n",
    "            fields.append(str_struct)\n",
    "            #_types.append((_schema[key], data_type))\n",
    "            print 'key:', '\"'+_schema[key]+'\",',',',' value:', _keys[1][key],',', ' data type:', data_type #, ' value:', _keys[0][key],\n",
    "        \n",
    "        elif (type(_keys[1][key]) == unicode) or (type(_keys[0][key]) == str):\n",
    "            data_type = StringType()\n",
    "            str_struct = StructField(_schema[key], data_type, True)\n",
    "            fields.append(str_struct)\n",
    "            #_types.append((_schema[key], data_type))\n",
    "            print 'key:', '\"'+_schema[key]+'\",',',', ' value:', _keys[1][key], ',', 'data type:', data_type\n",
    "        \n",
    "        else:# (type(_keys[0][key]) == unicode) or (type(_keys[0][key]) == str):\n",
    "            data_type = TimestampType()\n",
    "            str_struct = StructField(_schema[key], data_type, True)\n",
    "            fields.append(str_struct)\n",
    "            #_types.append((_schema[key], data_type))\n",
    "            print 'key:', '\"'+_schema[key]+'\",',',', ' value:', _keys[1][key], ',', 'data type:', data_type\n",
    "    \n",
    "    print \"------------------------------------------\"\n",
    "    print \"Your schema's 'StructField' are:\"\n",
    "    for x in fields:\n",
    "        print x\n",
    "    \n",
    "    print \"------------------------------------------\"\n",
    "\n",
    "    \n",
    "    # Creating the spark DataFrame:\n",
    "    conf = spark_session(version=0)\n",
    "    sc = SparkContext.getOrCreate(conf = conf)\n",
    "    _spark_data.map(lambda l: tuple(l[:1]) + tuple(l[1].split(','))).map(lambda x: x.split('\\t'))\n",
    "    print _spark_data\n",
    "    #rdd = sc.parallelize(_spark_data.map( lambda elem: list(elem)))\n",
    "    #.map(lambda p: (p[0], p[1].strip()))\n",
    "    \n",
    "    schema = StructType(fields)\n",
    "    spark_df = _spark_data.map(lambda x: (x, )).toDF(schema)\n",
    "#     spark_df.head(5)\n",
    "    \n",
    "#     try:\n",
    "#         conf = spark_session(version=0)\n",
    "#         sc = SparkContext.getOrCreate(conf = conf)\n",
    "#         rdd = sc.parallelize(spark_data)\n",
    "    #spark_df = spark_session(version=1).createDataFrame(spark_data,schema)\n",
    "#     spark_df.show()\n",
    "#     except Exception as e:\n",
    "#         print (e)\n",
    "    \n",
    "\n",
    "sample = \"/Users/Ahmed/Documents/DataMining_Stuff/Hadoop/Spark/PySpark/data/airbnb/sample/sample.csv\"\n",
    "# spark_data_txt(sample)\n",
    "# spark_data_sql(sample)\n",
    "residual_data()\n",
    "\n",
    "#spark_schema_v0(sample)\n",
    "#spark_schema_v1(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
